{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "18d34e37-5b68-410b-86f8-fdbf25903dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estas son mis credenciales para Snowflake: {'sfURL': 'LSNDJXB-RHC82043.snowflakecomputing.com', 'sfUser': 'usuario_spark', 'sfPassword': 'EstudianteEstudiante64', 'sfDatabase': 'NY_TAXI', 'sfSchema': 'raw', 'sfWarehouse': 'WAREHOUSE_TAXIS', 'sfRole': 'rol_pocos_privilegios'}\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Esto carga las variables desde tu archivo .env en el directorio actual\n",
    "load_dotenv()\n",
    "\n",
    "credencialesSnowflakeRaw = {\n",
    "    \"sfURL\" : os.getenv(\"SNOWFLAKE_ACCOUNT\"),\n",
    "    \"sfUser\" :  os.getenv(\"SNOWFLAKE_USER\"),\n",
    "    \"sfPassword\" : os.getenv(\"SNOWFLAKE_PASSWORD\"),\n",
    "    \"sfDatabase\" : os.getenv(\"SNOWFLAKE_DATABASE\"),\n",
    "    \"sfSchema\" : os.getenv(\"SNOWFLAKE_SCHEMA_RAW\"),\n",
    "    \"sfWarehouse\" : os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfRole\" : os.getenv(\"SNOWFLAKE_ROLE\"),\n",
    "}\n",
    "\n",
    "print(f\"Estas son mis credenciales para Snowflake: {credencialesSnowflakeRaw}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc7d55fc-3dff-4e79-8837-7304a6329bc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x72abedd23cd0>\n",
      "Spark Version : 3.5.0\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Crear SparkSession para conexión con Snowflake\n",
    "spark = (SparkSession.builder.appName(\"IngestaNewYorkTaxis\").config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc:3.13.30,net.snowflake:spark-snowflake_2.12:2.9.0-spark_3.1\").config(\"spark.sql.catalog.snowflake\", \"org.apache.spark.sql.execution.datasources.v2.snowflake.SnowflakeCatalog\").config(\"spark.sql.catalog.snowflake.sfURL\", credencialesSnowflake[\"sfURL\"]).config(\"spark.sql.catalog.snowflake.sfUser\", credencialesSnowflake[\"sfUser\"]).config(\"spark.sql.catalog.snowflake.sfPassword\", credencialesSnowflake[\"sfPassword\"]).config(\"spark.sql.catalog.snowflake.sfDatabase\", credencialesSnowflake[\"sfDatabase\"]).config(\"spark.sql.catalog.snowflake.sfSchema\", credencialesSnowflake[\"sfSchema\"]).config(\"spark.sql.catalog.snowflake.sfWarehouse\", credencialesSnowflake[\"sfWarehouse\"]).config(\"spark.sql.catalog.snowflake.sfRole\", credencialesSnowflake[\"sfRole\"]).getOrCreate())\n",
    "\n",
    "print(spark)\n",
    "print(\"Spark Version : \" + spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "da57d16b-fb85-4b5a-b0c4-b78fb7d11ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (1.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b99d7953-5c9d-4a4f-95f7-b1186746a897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def crear_tabla_raw_taxis(service: str):\n",
    "    crear_tabla_taxis = f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS NY_TAXI_RAW_{service.upper()} (\n",
    "        VENDORID INT,\n",
    "        TPEP_PICKUP_DATETIME TIMESTAMP_NTZ,\n",
    "        TPEP_DROPOFF_DATETIME TIMESTAMP_NTZ,\n",
    "        PASSENGER_COUNT INT,\n",
    "        TRIP_DISTANCE FLOAT,\n",
    "        RATECODEID INT,\n",
    "        STORE_AND_FWD_FLAG STRING,\n",
    "        PULOCATIONID INT,\n",
    "        DOLOCATIONID INT,\n",
    "        PAYMENT_TYPE INT,\n",
    "        FARE_AMOUNT FLOAT,\n",
    "        EXTRA FLOAT,\n",
    "        MTA_TAX FLOAT,\n",
    "        TIP_AMOUNT FLOAT,\n",
    "        TOLLS_AMOUNT FLOAT,\n",
    "        IMPROVEMENT_SURCHARGE FLOAT,\n",
    "        TOTAL_AMOUNT FLOAT,\n",
    "        CONGESTION_SURCHARGE FLOAT,\n",
    "        AIRPORT_FEE FLOAT,\n",
    "        RUN_ID STRING,\n",
    "        SERVICE_TYPE STRING,\n",
    "        SOURCE_YEAR INT,\n",
    "        SOURCE_MONTH INT,\n",
    "        INGESTED_AT_UTC TIMESTAMP_NTZ,\n",
    "        SOURCE_PATH STRING\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    #Genero la tabla desde la presente funcion con primary key compuesta para idempotencia\n",
    "    try:\n",
    "        spark.sql(crear_tabla_taxis)\n",
    "        print(f\"Tabla NY_TAXI_RAW_{service.upper()} creada correctamente en Snowflake\")\n",
    "    except Exception as e:\n",
    "        print(f\"Fallo la creacion de la tabla de taxis: {e}\")\n",
    "    else:\n",
    "        print(\"Tabla NY_TAXI_RAW creada correctamente en Snowflake\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "61f459b5-1d60-4cb1-be75-a56f751238c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "#Hago la presente funcion para generar un identificador unico asociado a cada carga de datos para el RUN_ID \n",
    "def generar_run_id():\n",
    "    return datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7a949fad-dfe5-487c-959b-8873bab2f94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from pyspark.sql.functions import lit, current_timestamp, to_utc_timestamp\n",
    "from pyspark.sql.types import TimestampType\n",
    "\n",
    "def ingestar_parquet_a_raw(service: str, year: int, month: int):\n",
    "    SOURCE_PATH=os.getenv(\"SOURCE_PATH\")\n",
    "    path_url = f\"{SOURCE_PATH}/trip-data/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    local_path = f\"/tmp/{service}_tripdata_{year}-{month:02d}.parquet\"\n",
    "    \n",
    "    # Descargar el archivo parquet en carpeta temporal para ingesta de datos\n",
    "    try:\n",
    "        r = requests.get(path_url, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            with open(local_path, 'wb') as f:\n",
    "                for chunk in r.iter_content(chunk_size=10000000):\n",
    "                    f.write(chunk)\n",
    "        else:\n",
    "            print(f\"Archivo no encontrado en {path_url} (status {r.status_code})\")\n",
    "            return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error descargando {path_url}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo obtenido existosamente de: {path_url}\")\n",
    "    \n",
    "    # Leo el archivo parquet localmente\n",
    "    try:\n",
    "        df = spark.read.parquet(local_path)\n",
    "    except Exception as e:\n",
    "        print(f\"No se pudo leer {local_path}: {e}\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Archivo leído existosamente por Spark: {local_path}\")\n",
    "\n",
    "    run_id = generar_run_id()\n",
    "\n",
    "    #Elimino nombre de columna presente en pocos archivos\n",
    "    if 'cbd_congestion_fee' in df.columns:\n",
    "        df = df.drop('cbd_congestion_fee')\n",
    "\n",
    "    #Homogenizo nombres de columnas a mayusculas para evitar errores en ingesta\n",
    "    df = df.toDF(*[c.upper() for c in df.columns])\n",
    "\n",
    "    # Añadir metadatos\n",
    "    df_meta = df.withColumn(\"run_id\", lit(run_id)) \\\n",
    "                .withColumn(\"service_type\", lit(service)) \\\n",
    "                .withColumn(\"source_year\", lit(year)) \\\n",
    "                .withColumn(\"source_month\", lit(month)) \\\n",
    "                .withColumn(\"ingested_at_utc\", to_utc_timestamp(current_timestamp(), 'UTC')) \\\n",
    "                .withColumn(\"source_path\", lit(path_url))\n",
    "\n",
    "    \n",
    "    for field in df_meta.schema.fields:\n",
    "        if field.dataType.typeName() == \"timestamp_ntz\":\n",
    "            df_meta = df_meta.withColumn(field.name, df_meta[field.name].cast(TimestampType()))\n",
    "\n",
    "    \n",
    "    conteoFilas = df_meta.count()\n",
    "    print(f\"Ingestando hacia Snowflake {service} {year}-{month}. Total de filas: {conteoFilas}\")\n",
    "\n",
    "    # Creo una vista temporal para los nuevos datos\n",
    "    df_meta.createOrReplaceTempView(\"nueva_data\")\n",
    "\n",
    "    # Ejecuto el Merge para actualizar o insertar los datos en Snowflake y de esa forma asegurar idempotencia\n",
    "    # Aplico validacion con combinacion de columnas para emular primary key compuesta\n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO NY_TAXI_RAW_{service} AS target\n",
    "    USING nueva_data AS source\n",
    "    ON target.VENDORID = source.VENDORID \n",
    "       AND target.TPEP_PICKUP_DATETIME = source.TPEP_PICKUP_DATETIME \n",
    "       AND target.TPEP_DROPOFF_DATETIME = source.TPEP_DROPOFF_DATETIME\n",
    "       AND target.RATECODEID = source.RATECODEID\n",
    "       AND target.PULOCATIONID = source.PULOCATIONID\n",
    "       AND target.DOLOCATIONID = source.DOLOCATIONID\n",
    "    WHEN MATCHED THEN\n",
    "        UPDATE SET\n",
    "            target.PASSENGER_COUNT = source.PASSENGER_COUNT,\n",
    "            target.TRIP_DISTANCE = source.TRIP_DISTANCE,\n",
    "            target.STORE_AND_FWD_FLAG = source.STORE_AND_FWD_FLAG,\n",
    "            target.PAYMENT_TYPE = source.PAYMENT_TYPE,\n",
    "            target.FARE_AMOUNT = source.FARE_AMOUNT,\n",
    "            target.EXTRA = source.EXTRA,\n",
    "            target.MTA_TAX = source.MTA_TAX,\n",
    "            target.TIP_AMOUNT = source.TIP_AMOUNT,\n",
    "            target.TOLLS_AMOUNT = source.TOLLS_AMOUNT,\n",
    "            target.IMPROVEMENT_SURCHARGE = source.IMPROVEMENT_SURCHARGE,\n",
    "            target.TOTAL_AMOUNT = source.TOTAL_AMOUNT,\n",
    "            target.CONGESTION_SURCHARGE = source.CONGESTION_SURCHARGE,\n",
    "            target.AIRPORT_FEE = source.AIRPORT_FEE,\n",
    "            target.RUN_ID = source.RUN_ID,\n",
    "            target.SERVICE_TYPE = source.SERVICE_TYPE,\n",
    "            target.SOURCE_YEAR = source.SOURCE_YEAR,\n",
    "            target.SOURCE_MONTH = source.SOURCE_MONTH,\n",
    "            target.INGESTED_AT_UTC = source.INGESTED_AT_UTC,\n",
    "            target.SOURCE_PATH = source.SOURCE_PATH\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT (\n",
    "            VENDORID, TPEP_PICKUP_DATETIME, TPEP_DROPOFF_DATETIME, PASSENGER_COUNT, TRIP_DISTANCE, RATECODEID,\n",
    "            STORE_AND_FWD_FLAG,PULOCATIONID,DOLOCATIONID,PAYMENT_TYPE,FARE_AMOUNT,EXTRA,MTA_TAX,TIP_AMOUNT,\n",
    "            TOLLS_AMOUNT,IMPROVEMENT_SURCHARGE,TOTAL_AMOUNT,CONGESTION_SURCHARGE,AIRPORT_FEE,RUN_ID,\n",
    "            SERVICE_TYPE,SOURCE_YEAR,SOURCE_MONTH,INGESTED_AT_UTC,SOURCE_PATH\n",
    "        ) VALUES (\n",
    "            source.VENDORID, source.TPEP_PICKUP_DATETIME, source.TPEP_DROPOFF_DATETIME, source.PASSENGER_COUNT, \n",
    "            source.TRIP_DISTANCE, source.RATECODEID, source.STORE_AND_FWD_FLAG, source.PULOCATIONID,\n",
    "            source.DOLOCATIONID, source.PAYMENT_TYPE, source.FARE_AMOUNT, source.EXTRA,\n",
    "            source.MTA_TAX, source.TIP_AMOUNT, source.TOLLS_AMOUNT, source.IMPROVEMENT_SURCHARGE,\n",
    "            source.TOTAL_AMOUNT, source.CONGESTION_SURCHARGE, source.AIRPORT_FEE, source.RUN_ID,\n",
    "            source.SERVICE_TYPE, source.SOURCE_YEAR, source.SOURCE_MONTH, source.INGESTED_AT_UTC, source.SOURCE_PATH\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        spark.sql(merge_query)\n",
    "        print(f\"Datos de {service} {year}-{month} actualizados o insertados correctamente.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al ejecutar Merge en Snowflake: {e}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        os.remove(local_path)\n",
    "        print(f\"Archivo parquet temporal removido: {local_path}\")\n",
    "    except OSError as e:\n",
    "        print(f\"No se pudo remover el archivo parquet temporal {local_path}: {e}\")\n",
    "\n",
    "    return {\n",
    "        \"year\": year,\n",
    "        \"month\": month,\n",
    "        \"count\": conteoFilas\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ce5de8bc-2237-41c6-8390-64234e74ca44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['yellow', 'green']\n",
      "[2015, 2016, 2017, 2018, 2019, 2020, 2021, 2022, 2023, 2024, 2025]\n",
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n"
     ]
    }
   ],
   "source": [
    "tipos_taxis=os.getenv(\"SERVICES\").split(',')\n",
    "lista_years=os.getenv(\"YEARS\").split(',')\n",
    "lista_months=os.getenv(\"MONTHS\").split(',')\n",
    "lista_years = [int(item) for item in lista_years]\n",
    "lista_months = [int(item) for item in lista_months]\n",
    "\n",
    "print(tipos_taxis)\n",
    "print(lista_years)\n",
    "print(lista_months)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c94498a3-d522-4435-92fa-eb350604612a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fallo la creacion de la tabla de taxis: [NOT_SUPPORTED_COMMAND_WITHOUT_HIVE_SUPPORT] CREATE Hive TABLE (AS SELECT) is not supported, if you want to enable it, please set \"spark.sql.catalogImplementation\" to \"hive\".;\n",
      "'CreateTable `spark_catalog`.`default`.`NY_TAXI_RAW_YELLOW`, org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe, Ignore\n",
      "\n",
      "Fallo el proceso de ingesta masiva de datos de taxis NY: 0 is not in list\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "def save_checkpoint(year, month):\n",
    "    with open(CHECKPOINT_FILE, \"w\") as f:\n",
    "        json.dump({\"year\": year, \"month\": month}, f)\n",
    "\n",
    "def load_checkpoint(CHECKPOINT_FILE):\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        with open(CHECKPOINT_FILE, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {\"year\": 0, \"month\": 0}\n",
    "\n",
    "resultadosGeneralesIngesta=[]\n",
    "\n",
    "try:\n",
    "    for tipo_taxi in tipos_taxis:\n",
    "        CHECKPOINT_FILE = f\"checkpointTaxis{tipo_taxi.capitalize()}.json\"\n",
    "        crear_tabla_raw_taxis(tipo_taxi)\n",
    "        checkpoint=load_checkpoint(CHECKPOINT_FILE)\n",
    "        lista_years= lista_years[lista_years.index(checkpoint[\"year\"]):]\n",
    "        lista_months= lista_months[lista_months.index(checkpoint[\"month\"])+1:]\n",
    "\n",
    "        for year_taxi in lista_years:\n",
    "            \n",
    "            for month_taxi in lista_months:  \n",
    "                print(f\"Iniciando ingesta de datos de taxis {tipo_taxi}: {month_taxi}-{year_taxi}\")\n",
    "                resultadosParciales=ingest_parquet_to_raw(tipo_taxi, year_taxi, month_taxi)\n",
    "                resultadosGeneralesIngesta.append(resultadosParciales)\n",
    "                save_checkpoint(year_taxi,month_taxi)\n",
    "                \n",
    "except Exception as e:\n",
    "    print(f\"Fallo el proceso de ingesta masiva de datos de taxis NY: {e}\")\n",
    "else:\n",
    "    print(\"El proceso de ingesta masiva de taxis NY fue exitoso\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4076e6d7-6840-46fc-9a3a-5ab56eeb5848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "# Configuración de credenciales de Snowflake\n",
    "credencialesSnowflake = {\n",
    "    \"sfURL\": os.getenv(\"SNOWFLAKE_URL\", \"tu_account.snowflakecomputing.com\"),\n",
    "    \"sfUser\": os.getenv(\"SNOWFLAKE_USER\", \"tu_usuario\"),\n",
    "    \"sfPassword\": os.getenv(\"SNOWFLAKE_PASSWORD\", \"tu_password\"),\n",
    "    \"sfDatabase\": os.getenv(\"SNOWFLAKE_DATABASE\", \"NY_TAXI_DB\"),\n",
    "    \"sfSchema\": os.getenv(\"SNOWFLAKE_SCHEMA\"),\n",
    "    \"sfWarehouse\": os.getenv(\"SNOWFLAKE_WAREHOUSE\"),\n",
    "    \"sfRole\": os.getenv(\"SNOWFLAKE_ROLE\", \"\")\n",
    "}\n",
    "\n",
    "# Crear SparkSession para conexión con Snowflake - SIN configurar catálogo\n",
    "spark = (SparkSession.builder\n",
    "    .appName(\"IngestaNewYorkTaxis\")\n",
    "    .config(\"spark.jars.packages\", \"net.snowflake:snowflake-jdbc:3.13.30,net.snowflake:spark-snowflake_2.12:2.9.0-spark_3.1\")\n",
    "    # Removemos la configuración del catálogo para evitar conflictos con Hive\n",
    "    .getOrCreate())\n",
    "\n",
    "# Configuraciones adicionales de Spark para mejor performance\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "\n",
    "print(spark)\n",
    "print(\"Spark Version: \" + spark.version)\n",
    "print(\"✓ SparkSession configurada exitosamente\")\n",
    "\n",
    "# Función para verificar la conexión a Snowflake\n",
    "def verificar_conexion_snowflake():\n",
    "    \"\"\"\n",
    "    Verifica que la conexión a Snowflake funcione correctamente\n",
    "    \"\"\"\n",
    "    try:\n",
    "        test_query = \"SELECT CURRENT_TIMESTAMP() as current_time, CURRENT_VERSION() as version\"\n",
    "        \n",
    "        df_test = spark.read \\\n",
    "            .format(\"snowflake\") \\\n",
    "            .options(**credencialesSnowflake) \\\n",
    "            .option(\"query\", test_query) \\\n",
    "            .load()\n",
    "        \n",
    "        resultado = df_test.collect()[0]\n",
    "        print(f\"✓ Conexión a Snowflake exitosa\")\n",
    "        print(f\"  Hora actual en Snowflake: {resultado['current_time']}\")\n",
    "        print(f\"  Versión de Snowflake: {resultado['version']}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error en la conexión a Snowflake: {e}\")\n",
    "        return False\n",
    "\n",
    "verificar_conexion_snowflake()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e33c84-9fdf-45d2-a552-99677489017c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
